# Pinecone Configuration
# Get your API key from: https://app.pinecone.io/
PINECONE_API_KEY=pcsk_...
# Your Pinecone environment (e.g., gcp-starter, us-west1-gcp)
PINECONE_ENVIRONMENT=us-east1-gcp
# Name for your vector index
PINECONE_INDEX_NAME=your-index-name-here

OPENAI_API_KEY=sk-proj-_...

# # Google Cloud credentials
GOOGLE_APPLICATION_CREDENTIALS=/home/rgenet/gitl/pinecone/.secrets/google-credentials.json
GOOGLE_PROJECT_ID=prizmpoc
GOOGLE_CLIENT_EMAIL=svcacct1@prizmpoc.iam.gserviceaccount.com

# Model Configuration
# Available models: gemini-pro, gemini-pro-vision
MODEL_NAME=gemini-pro
# Embedding model for vector creation
EMBEDDING_MODEL=text-embedding-004

# Document Processing Settings
# Size of text chunks when splitting documents
CHUNK_SIZE=1000
# Overlap between chunks to maintain context
CHUNK_OVERLAP=200

# Optional: Application Settings
# Temperature for LLM responses (0.0 to 1.0)
TEMPERATURE=0.7
# Maximum number of tokens in response
MAX_OUTPUT_TOKENS=2048 
